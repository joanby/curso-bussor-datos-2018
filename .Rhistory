tt
prop.table(tt)
pr <- prediction(pred, bank[-idx, ]$class)
pr <- prediction(pred[,2], bank[-idx, ]$class)
pred[,2]
pred[,2]
pred[,2]
pred
pred <- predict(tree.pruned, bank[-idx, ], type = "prob")
pred
pr <- prediction(pred[,2], bank[-idx, ]$class)
perf <- performance(pr, "tpr", "fpr")
plot(perf)
install.packages("randomForest")
library(randomForest)
bank
bank$class <- factor(bank$class)
bank
forest <- randomForest(x = bank[idx, 1:4],
y = bank[idx, ]$class,
ntree = 500,
keep.forest = TRUE)
forest
pred <- predict(forest, bank[-idx, ])
table(bank[-idx,]$class, pred, dnn = c("Real", "Pred"))
?predict
pred <- predict(forest, bank[-idx, ], type = "prob")
pred
perf <- performance(pred, "tpr", "fpr")
pred
prediction <- prediction(pred[,2], bank[-idx,]$class)
perf <- performance(prediction, "tpr", "fpr")
plot(perf)
forest <- randomForest(x = bank[idx, 1:4],
y = bank[idx, ]$class,
ntree = 5000,
keep.forest = TRUE)
forest
pred <- predict(forest, bank[-idx, ], type = "class")
table(bank[-idx,]$class, pred, dnn = c("Real", "Pred"))
pred <- predict(forest, bank[-idx, ], type = "prob")
pred
prediction <- prediction(pred[,2], bank[-idx,]$class)
perf <- performance(prediction, "tpr", "fpr")
plot(perf)
forest <- randomForest(x = bank[idx, 1:4],
y = bank[idx, ]$class,
ntree = 100,
keep.forest = TRUE)
forest
pred <- predict(forest, bank[-idx, ], type = "class")
table(bank[-idx,]$class, pred, dnn = c("Real", "Pred"))
forest <- randomForest(x = bank[idx, 1:4],
y = bank[idx, ]$class,
ntree = 10,
keep.forest = TRUE)
forest
pred <- predict(forest, bank[-idx, ], type = "class")
table(bank[-idx,]$class, pred, dnn = c("Real", "Pred"))
pred <- predict(forest, bank[-idx, ], type = "prob")
pred
prediction <- prediction(pred[,2], bank[-idx,]$class)
perf <- performance(prediction, "tpr", "fpr")
plot(perf)
install.packages("e1071")
??e1071
knitr::opts_chunk$set(echo = TRUE)
bank <- read.csv("../data/tema3/banknote-authentication.csv")
bank$class <- factor(bank$class)
set.seed(2018)
library(e1071)
model <- svm(class ~ ., data = bank[idx, ])
table(bank[idx,]$class, fitted(model), dnn = c("Actual", "Predicción"))
idx <- createDataPartition(bank$class, p = 0.8, list = F)
set.seed(2018)
idx <- createDataPartition(bank$class, p = 0.8, list = F)
model <- svm(class ~ ., data = bank[idx, ])
table(bank[idx,]$class, fitted(model), dnn = c("Actual", "Predicción"))
pred <- predict(model, bank[-idx,])
table(bank[-idx,]$class, pred, dnn = c("Actual", "Predicción"))
plot(model, data = bank[idx, ], variance ~ skew )
plot(model, data = bank[-idx, ], variance ~ skew )
plot(model, data = bank[-idx, ], variance ~ curtosis )
plot(model, data = bank[-idx, ], variance ~ entropy )
?svm
tune <- tune.svm(class ~ ., data = bank[idx, ], gamma = 10^(-6:-1), cost = 10^(1:2))
summary(tune)
purchase <- read.csv("../data/tema3/electronics-purchase.csv")
View(purchase)
set.seed(2018)
idx <- createDataPartition(purchase$Purchase, p = 0.66, list = FALSE)
bayes <- naiveBayes(Purchase ~ ., data = purchase[idx, ])
bayes
pred <- predict(bayes, purchase[-idx,])
table(purchase[-idx,]$Purchase, pred, dnn = c("Actual", "Predicción"))
tt <- table(purchase[-idx,]$Purchase, pred, dnn = c("Actual", "Predicción"))
confusionMatrix(tt)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(class)
vac <- read.csv("../data/tema3/vacation-trip-classification.csv")
View(vac)
vac$Income.Z = scale(vac$Income)
vac$Family_size.Z = scale(vac$Family_size)
idx <- createDataPartition(vac$Result, p = 0.5, list = FALSE)
train <- vac[idx,]
temp <- vac[-idx,]
idx2 <- createDataPartition(temp$Result, p= 0.5, list = FALSE)
val <- temp[idx2,]
test <- temp[-idx2,]
pred1 <- knn(train[,4:5], val[,4:5], train$Result, k = 1)
pred1
table(val$Result, pred1, dnn = c("Actual", "Predicción"))
for(k in 1:8){
pred1 <- knn(train[,4:5], val[,4:5], train$Result, k = k)
tt<-table(val$Result, pred1, dnn = c("Actual", "Predicción"))
cat(paste("Matriz de confusión para k = ", k, "\n"))
cat("=============================================\n")
print(tt)
cat("=============================================\n\n\n")
}
trControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
View(trControl)
knn <- train(Result~Family_size + Income, data = train, method = "knn",
trControl = trControl, preProcess = c("center", "scale"),
tuneLength = 10)
knn
pred1 <- knn(train[,4:5], val[,4:5], train$Result, k = 17)
tt<-table(val$Result, pred1, dnn = c("Actual", "Predicción"))
tt
set.seed(2018)
trControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
knn <- train(Result~Family_size + Income, data = train, method = "knn",
trControl = trControl, preProcess = c("center", "scale"),
tuneLength = 10)
knn
pred1 <- knn(train[,4:5], val[,4:5], train$Result, k = 17)
tt<-table(val$Result, pred1, dnn = c("Actual", "Predicción"))
tt
knitr::opts_chunk$set(echo = TRUE)
rmse <- function(actual, prediction){
return (sqrt(mean((actual-prediction)^2)))
}
data <- read.csv("../data/tema4/rmse.csv")
View(data)
rmse(data$price, data$pred)
rmse(data$price, data$pred)/mean(data$price)
rmse(data$price, data$pred)/mean(data$price)*100
return(rmse(actual,  prediction)/mean(actual)*100)
relative.error <-function(actual, prediction){
return(rmse(actual,  prediction)/mean(actual)*100)
}
relative.error(data$price, data$pred)
rmse(data$price, data$pred)
relative.error(data$price, data$pred)
View(rmse)
install.packages("FNN")
knitr::opts_chunk$set(echo = TRUE)
library(FNN)
library(dummies)
library(caret)
library(scales)
edu <- read.csv("../data/tema4/education.csv")
View(edu)
dms <- dummy(edu$region, sep = "_")
View(dms)
edu <- cbind(edu, dms)
edu$urban.Z <- rescale(edu$urban)
edu$income.Z <- rescale(edu$income)
edu$under18.Z <- rescale(edu$under18)
tr <- edu[ids, ]
ids <- createDataPartition(edu$expense, p = 0.6, list = FALSE)
tr <- edu[ids, ]
temp<- edu[-ids,]
ids2 <- createDataPartition(temp, p=0.5, list = FALSE)
ids2 <- createDataPartition(temp$expense, p=0.5, list = FALSE)
val <- temp[ids2,]
test<- temp[-ids2,]
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
print(rmse(res$pred, val$expense))
}
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
print(k)
print("==============")
print(rmse(res$pred, val$expense))
}
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
print(cat("K=",k))
print("==============")
print(rmse(res$pred, val$expense))
print("\n\n")
}
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
print(cat("K=",k,""))
print("==============")
print(rmse(res$pred, val$expense))
print("\n\n")
}
cat("\n\n")
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
print(cat("K=",k,""))
print("==============")
print(rmse(res$pred, val$expense))
cat("\n\n")
}
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
cat("K=",k,"")
print("==============")
print(rmse(res$pred, val$expense))
cat("\n\n")
}
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
cat("K=",k,"\n")
print("==============")
print(rmse(res$pred, val$expense))
cat("\n\n")
}
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
cat("K=",k,"\n")
print("==============")
print(rmse(res$pred, val$expense))
cat("\n\n")
}
errors <- vector()
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
cat("K=",k,"\n")
print("==============")
rr <- rmse(res$pred, val$expense)
print(rr)
cat("\n\n")
errors <- c(errors, rr)
}
plot(errors, type = 'o', xlab = "k", ylab = "RMSE")
errors <- vector()
for(k in 2:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
cat("K=",k,"\n")
print("==============")
rr <- rmse(res$pred, val$expense)
print(rr)
cat("\n\n")
errors <- c(errors, rr)
}
plot(x = 2:10, y=errors, type = 'o', xlab = "k", ylab = "RMSE")
errors <- vector()
for(k in 1:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
cat("K=",k,"\n")
print("==============")
rr <- rmse(res$pred, val$expense)
print(rr)
cat("\n\n")
errors <- c(errors, rr)
}
plot(x = 1:10, y=errors, type = 'o', xlab = "k", ylab = "RMSE")
errors <- vector()
for(k in 1:10){
res <- knn.reg(tr[,7:13], val[,7:13], tr$expense, k, algorithm = "brute")
cat("K=",k,"\n")
print("==============")
rr <- relative.error(res$pred, val$expense)
print(rr)
cat("\n\n")
errors <- c(errors, rr)
}
plot(x = 1:10, y=errors, type = 'o', xlab = "k", ylab = "RMSE")
car <- read.csv("../data/tema4/auto-mpg.csv")
car$cylinders <- factor(car$cylinders)
View(car)
car
set.seed(2018)
idx <- createDataPartition(car$mpg, p = 0.7, list = FALSE)
mod <- lm(mpg~., data = car[ids, ])
mod <- lm(mpg~., data = car[ids, -c(1,8,9)])
mod
summary(mod)
pred <- predict(mod, car[-idx, -c(1,8,9)])
set.seed(2018)
idx <- createDataPartition(car$mpg, p = 0.7, list = FALSE)
mod <- lm(mpg~., data = car[ids, -c(1,8,9)])
mod
summary(mod)
pred <- predict(mod, car[-idx, -c(1,8,9)])
set.seed(2019)
idx <- createDataPartition(car$mpg, p = 0.7, list = FALSE)
mod <- lm(mpg~., data = car[ids, -c(1,8,9)])
mod
summary(mod)
pred <- predict(mod, car[-idx, -c(1,8,9)])
car <- read.csv("../data/tema4/auto-mpg.csv")
car$cylinders <- factor(car$cylinders)
car
set.seed(2019)
idx <- createDataPartition(car$mpg, p = 0.7, list = FALSE)
mod <- lm(mpg~., data = car[ids, -c(1,8,9)])
mod
set.seed(1988)
idx <- createDataPartition(car$mpg, p = 0.7, list = FALSE)
mod <- lm(mpg~., data = car[ids, -c(1,8,9)])
mod
summary(mod)
car$cylinders <- factor(car$cylinders)
car
car$cylinders <- factor(car$cylinders)
car
set.seed(1988)
idx <- createDataPartition(car$mpg, p = 0.9, list = FALSE)
mod <- lm(mpg~., data = car[ids, -c(1,8,9)])
mod
summary(mod)
pred <- predict(mod, car[-idx, -c(1,8,9)])
pred
dms <- dummies(car$cylinders)
library(dummies)
dms <- dummies(car$cylinders)
dms <- dummy(car$cylinders)
View(dms)
car <- cbind(car, dms)
car
set.seed(1988)
idx <- createDataPartition(car$mpg, p = 0.7, list = FALSE)
mod <- lm(mpg~., data = car[ids, -c(1,3,8,9)])
mod
summary(mod)
pred <- predict(mod, car[-idx, -c(1,8,9)])
pred
pred <- predict(mod, car[-idx, -c(1,8,9)])
set.seed(2018)
idx <- createDataPartition(car$mpg, p = 0.7, list = FALSE)
mod <- lm(mpg~., data = car[ids, -c(1,3,8,9)])
mod
summary(mod)
pred <- predict(mod, car[-idx, -c(1,8,9)])
pred
plot(mod)
par(mfrow=c(2,2))
plot(mod)
plot(mod)
mod <- lm(log(mpg)~., data = car[ids, -c(1,3,8,9)])
mod
summary(mod)
pred <- predict(mod, car[-idx, -c(1,8,9)])
pred
par(mfrow=c(2,2))
plot(mod)
mod <- lm(mpg~., data = car[ids, -c(1,3,8,9)])
mod
library(MASS)
step <- stepAIC(mod, direction = "backward")
car[-idx, -c(1,8,9)]
library(caret)
library(rpart)
library(rpart.plot)
boston <- read.csv("../data/tema4/BostonHousing.csv")
View(boston)
set.seed(2018)
ids <- createDataPartition(boston$MEDV, p = 0.7, list = FALSE)
tree <- rpart(MEDV ~ ., data = boston[ids, ])
tree
prp(tree, type = 2, nn = TRUE, fallen.leaves =  TRUE, faceln = 4,
varlen = 8, shadow.col = "gray")
prp(tree, type = 2, nn = TRUE, fallen.leaves =  TRUE, faclen = 4,
varlen = 8, shadow.col = "gray")
tree$cptable
plotcp(tree)
tree$cptable
tree.pruned <- prune(tree, cp = 0.03)
prp(tree.pruned, type = 2, nn = TRUE, fallen.leaves =  TRUE, faclen = 4,
varlen = 8, shadow.col = "gray")
pred <- predict(tree.pruned, boston[-ids,])
rmse(pred, boston[-ids,]$MEDV)
relative.error(pred, boston[-ids,]$MEDV)
library(randomForest)
forest <- randomForest(x = boston[ids, -c(1)], y = boston[ids,]$MEDV,
xtest = boston[-ids, -c(1)], ytest = boston[-ids,]$MEDV,
ntree = 1000, importance = TRUE, keep.forest = TRUE)
forest
forest$importance
plot(boston[ids, ]$MEDV, predict(mod, newdata = boston[ids,-c(1)]), xlab = "Actual", ylab = "Predicción" )
plot(boston[ids, ]$MEDV, predict(forest, newdata = boston[ids,-c(1)]), xlab = "Actual", ylab = "Predicción" )
plot(boston[-ids, ]$MEDV, predict(forest, newdata = boston[-ids,-c(1)]), xlab = "Actual", ylab = "Predicción" )
forest <- randomForest(x = boston[ids, -c(1)], y = boston[ids,]$MEDV,
xtest = boston[-ids, -c(1)], ytest = boston[-ids,]$MEDV,
ntree = 10000, importance = TRUE, keep.forest = TRUE)
forest
forest$importance
plot(boston[ids, ]$MEDV, predict(forest, newdata = boston[ids,-c(1)]), xlab = "Actual", ylab = "Predicción" )
plot(boston[-ids, ]$MEDV, predict(forest, newdata = boston[-ids,-c(1)]), xlab = "Actual", ylab = "Predicción" )
forest <- randomForest(x = boston[ids, -c(1)], y = boston[ids,]$MEDV,
xtest = boston[-ids, -c(1)], ytest = boston[-ids,]$MEDV,
ntree = 500, importance = TRUE, keep.forest = TRUE)
forest
forest$importance
plot(boston[ids, ]$MEDV, predict(forest, newdata = boston[ids,-c(1)]), xlab = "Actual", ylab = "Predicción" )
plot(boston[-ids, ]$MEDV, predict(forest, newdata = boston[-ids,-c(1)]), xlab = "Actual", ylab = "Predicción" )
kfold.cv <- function(df, k_folds){
fold <- sample(1:k_folds, nrow(df), replace = TRUE)
print(fold)
}
kfold.cv(boston, 5)
kfold.cv <- function(df, k_folds){
fold <- sample(1:k_folds, nrow(df), replace = TRUE)
for(k in 1:k_folds){
training.ids <- fold %in% c(k)
print(training.ids)
}
}
kfold.cv(boston, 5)
kfold.cv <- function(df, k_folds){
fold <- sample(1:k_folds, nrow(df), replace = TRUE)
for(k in 1:k_folds){
training.ids <- fold %in% c(k)
print(k)
print(training.ids)
}
}
kfold.cv(boston, 5)
kfold.cv <- function(df, k_folds){
fold <- sample(1:k_folds, nrow(df), replace = TRUE)
for(k in 1:k_folds){
training.ids <- !fold %in% c(k)
validation.ids <- fold %in% c(k)
mod <- lm(MEDV ~ ., data = df[training.ids,])
pred <- predict(mod, df[validation.ids,])
err <- rmse(pred, sdf[validation.ids,]$MEDV)
print(err)
}
}
kfold.cv(boston, 5)
kfold.cv <- function(df, k_folds){
fold <- sample(1:k_folds, nrow(df), replace = TRUE)
for(k in 1:k_folds){
training.ids <- !fold %in% c(k)
validation.ids <- fold %in% c(k)
mod <- lm(MEDV ~ ., data = df[training.ids,])
pred <- predict(mod, df[validation.ids,])
err <- rmse(pred, df[validation.ids,]$MEDV)
print(err)
}
}
kfold.cv(boston, 5)
kfold.cv(boston, 8)
kfold.cv <- function(df, k_folds){
fold <- sample(1:k_folds, nrow(df), replace = TRUE)
errors <- vector()
for(k in 1:k_folds){
training.ids <- !fold %in% c(k)
validation.ids <- fold %in% c(k)
mod <- lm(MEDV ~ ., data = df[training.ids,])
pred <- predict(mod, df[validation.ids,])
err <- rmse(pred, df[validation.ids,]$MEDV)
errors = c(errors, err)
}
print(errors)
print(mean(errors))
}
kfold.cv(boston, 8)
loo.cv <- function(df){
errors <- vector()
for(k in 1:nrow(df)){
mod <- lm(MEDV ~ ., data = df[-k,])
pred <- predict(mod, df[k,])
err <- rmse(pred, df[k,]$MEDV)
errors = c(errors, err)
}
print(errors)
print(mean(errors))
}
loo.cv(boston)
# Run the application
shinyApp(ui = ui, server = server)
library(shiny)
# Define UI for application that draws a histogram
ui <- fluidPage(
# Application title
titlePanel("Old Faithful Geyser Data"),
# Sidebar with a slider input for number of bins
sidebarLayout(
sidebarPanel(
sliderInput("bins",
"Number of bins:",
min = 1,
max = 50,
value = 30)
),
# Show a plot of the generated distribution
mainPanel(
plotOutput("distPlot")
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
output$distPlot <- renderPlot({
# generate bins based on input$bins from ui.R
x    <- faithful[, 2]
bins <- seq(min(x), max(x), length.out = input$bins + 1)
# draw the histogram with the specified number of bins
hist(x, breaks = bins, col = 'darkgray', border = 'white')
})
}
# Run the application
shinyApp(ui = ui, server = server)
